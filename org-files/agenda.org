* Agenda ADYL
** 07/10
*** worked on bandpass filter test
ie filter rare and too frequent users
- worked on the test case
- worked on integrating bandpass filter
filtering takes around 4500ms
** 09/10
*** TODO Successfully loaded tf model in Java ?
- Need to test if its working bc the input/output layers given by Alexis throw [IllegalArgumentException: No Operation named [serving_dense_3] in the Graph]
*** Need to write python script to draw graph from saved model
** 10/10
*** changed some tests in data-common

** 11/10
*** successfully loaded the tf model and got the result from the ouput layer
** 14/10
*** DONE need to batch predictions
    CLOSED: [2019-10-14 lun. 16:01]
*** DONE need to wrap tf in an akka actor
    CLOSED: [2019-10-14 lun. 16:01]
*** DONE set up akka cluster
    CLOSED: [2019-10-14 lun. 16:24]
** 17/10
*** got back on bandpass filtering
flatmapgroups(2000k) in ms
- 7000
groupBy time (200k) in ms
- 11014 Some(JOIN TIME 4981)
- reuse group DS : 16000 / 4000
- reuse group DS + persist them : 20000 / 5500
- reuse group DS + cache them : 18543 JOIN TIME 4914

** 21/10
*** DONE make test for ad transform job
    CLOSED: [2019-10-22 mar. 11:53]
*** DONE make null value for deprecated field in secor
    CLOSED: [2019-10-22 mar. 11:53]
** 22/10
*** DONE make test for FileReader / DatedFileReader
    CLOSED: [2019-10-23 mer. 09:34]
** 23/10
*** DONE use spark shell to write secor data in parquet
    CLOSED: [2019-10-23 mer. 17:55]
then compare with json to find if its faster
** 24/10
*** TODO make job in spark that convert from Auction to RTBTransaction in the same vein as transform ad server stat job
** 25/10
*** TODO find why spark cant explode with 11.4.1 snapshot
** 05/11
*** DONE remove cached file manager / azure FM / listRecursive + clean unused methods
    CLOSED: [2019-11-14 jeu. 11:03]
** 14/11
*** created class SqlWriter because we don't need a cloudFileManager that needs to be provided with FileWriter
** 2020
*** 2020-01 janvier
**** 2020-01-02 jeudi
***** fixed a broken regexp that broke in 2020
dateColumn=(201[0-9]-[0-9]*-[0-9]*) => dateColumn=(2[0-9]{3}-[0-9]*-[0-9]*)
    Entered on [2020-01-02 jeu. 15:44]
* Notes ADYL
** Release/PR
*** Before accepting a PR
- Check jenkins
*** Merging the PR
- Squash and commit
*** Making the release (dev -> prod)
- Check jenkins master
- make release
- jenkins
  - old view (arrow at the top)
  - tag
  - release num
  - schedule build
- update version in Deps.scala
Jenkins will update the Airflow DAGS, and the Jar in GCS.
** Test job
run cluster en local :
make create-cluster
make build
make push
./scripts/run_job.sh -s user -j [job]
** Launch shell
*** shell
- make create cluster
- make shell
*** gui
- make ssh tunnel
- make chrome
** Bugs
*** symbol error when make deploy airflow
remove spark function in job code
*** no filestystem for scheme "gs"
create a mock for the fileReader / fileWriter as it may have a GCSServiceWrapper
*** Option 'basePath' must be a directory
The cloud dir location (bucket probably) doesn't point to an existing folder (in /tmp) or on GCS.
Solution :
- set the correct cloud dir location (ie, look in conf, for ex bucket = "ayl-secor", path = "usa02/ssp_actions_json")
- verify that the config is correctly loaded
** Update a dag
- make build publish-dev deploy-dev
- in airflow dag : mark task as success
- wait a moment...
** Access old adserver stat
bucket : ayl-datawarehouse-adserver-stats-cold
* Notes Scala
** Akka
*** At most one msg processed per actor
https://doc.akka.io/docs/akka/current/guide/actors-intro.html
** Spark
*** UDF Error : Caused by: java.lang.ClassCastException: scala.collection.mutable.WrappedArray$ofRef cannot be cast to org.apache.spark.sql.Row
Structures (ie case classes) can be converted to Row by Spark.
So to create a UDF that takes a case class as argument, give it a row instead of the case class:
val eventCount = udf((eventKindPublishedAt: Row, eventKind: String) <- it works
val eventCount = udf((eventKindPublishedAt: EventKind, eventKind: String) <- doesn't work
In the same vein, to give as argument an array of struct :
val eventCount = udf((eventKindPublishedAt: Seq[Row], eventKind: String) <- works
val eventCount = udf((eventKindPublishedAt: Seq[EventKind], eventKind: String) <- doesn't work

Note : Row values can be extracted either by pattern matching or with _.getString / _.getInt / ...
* Notes
** Restart emacs launch as a daemon by systemctl
systemctl --user restart emacs.service
** Good img processor on Linux ImageMagick GUI (display q16)
* [OLD] Agenda Sportagraph
** 01/06
 done :
 added images to the mail
 sent mail to support : added support mail & support locale to conf
 global => {
             :support_email => 'support@sportagraph.com',
             :support_locale => 'fr_FR',

** 04/06
 done :
 spend time finding bug related to mongoDB => fixed with regis & aslam
 send mail to correct environment support (ie warning@development.sportagraph.com etc)
 cleaned up the code

** 05/06
 send mail to right ID manager + (instead of admins only) + added warned_min_right_id in conf
 changed the sender of the mails (now it's warning@development.sportagraph.com) and the branding
 added a link to the album containing the media in the mail

 to do :
 ref image preview to dam link ?
 send one email for batch nsfw content instead of N mails

** 08/06
 started to look into AWS
 merged story 1793 (apache poi -> tika for word text & metadata) into develop
 merged story 1585 (check that the branding of the requester account is the same than the one of the content pool)
 removed defaultLikelihood for nsfw warning, instead doesn't send any mail if there is no visionData

** 12/06
 upadated wiki about Rekog
 saw with regis how to modelizes the results to the rekog requests

** 13/06
 comparatif exhaustif GVision / Rekog

** 14/06
 now able to use rekog with DAM images, but infos is not stored

** 18/06
 store all the rekog data in scala classes
 => have to serialize it to pass it to mongoDB
 compilation issues : overhead GC when compiling

** 19/06
 serialization is completed => AWS results is now stored in mongoDB
 updated the wiki with the Gvision / Rekog comparison for two images

 Face collection
 how to put rekog in DB

** 20/06
 need to create mongo store for aws rekog
 need to store all the rekog data in that store
 need to store only a part of those datas in the media

** 21/06
 created the rekog store
 rekog data now stored in the store
 created an update fct that only update particuliar fields

** 22/06
 code refactoring
 fit AWS datas to store them in media.model
 create conf
 TODO : pb with ES naming, need to rince integration 1

** 23/06
 created pull request
 added calculation of facesAreaRatio
 régis rinsed ES int1 so i could change a field name
 adaptated rekog data to vision data so now almost 1:1 (except for NSFW results)

** 26/06
 send image to rekog using s3 link (via the media to be analyzed templateFormat, default templateFormat set in conf)  instead of byteArray
 allow disabling/enabling rekog with orga plugin
 allow disabling/enabling gvision with orga plugin
 allow disabling/enabling both with orga plugin

** 27/06
 start to work on rekog for videos
 trying to find how to avoid duplication of code because video requests share similarities with image requests
 trying to fit result model of images to videos²

** 28/06
 placeholder code for video rekog
 //todo
 Check TPS of 20 for  AWS rekog on videos

** 29/06
 trying to communicate with SQS and SNS
 redisExpiringDistributedSetDatabaseEndpoint not in worker conf

** 02/07
 code refactoring for rekog photo + merged pull request
 lots of bug encountered : clash of AWS version

** 03/07
 adapted code for image to start supporting videos
 conf issue
 added a queue in sporta AWS => trying to use SQS to get the result from AWS rekog
 blocked bc it seems that SNS doenst send message to queue => fixed by changing iam permission

** 04/07
 created worker to handle SQS message
 successfully deserialized SQS message => conversion status can be extracted

** 05/07
 TODO :
 handle conversion status to make a get request to the result
 to investigate : JobTag not present in the serialized SQS message but is indicated in the doc, maybe it's optional and depends on if we called the rekog request by specifying the job tag

 rekog worker changed from media worker to standalone worker
 created a module that will convert SQS notification message to rabbitMQ message to send them to the worker (worker only works with rabbitMQ messages)

** 06/07
 created conf for worker (queue url, SQS region)
 finished the module that sends SQS message to rabbitMQ
   => now the worker gets notified when rekognition has completed the video analyzes
 starting to work on handling the result we get from AWS

** 09/07
 rekog datas (unfiltered) are now stored in mongoDB
 merged last branch of develop to have rekog hotfix

** 10/07
 continue to filter & store rekog data to fit media model

** 11/07
 changed mongoDB update function
 refactored code so that we have startVideo -> onVideoComplete & startImage -> onImageComplete
   => they both send a message to the rekog worker who sends back a message to the service when it is possible to store the results
 rekog now works for video, we store the raw result in aws_rekognition and the filtered results in media.model.rekog
 updated wiki
 code refactoring to split getting the AWS result and writing to mongoDB

** 12/07
 fix pb with facesAreaRatio for videos ( > 1 )
 see if there is a way to enhance rekognition on video
 check if we can change analysis framerate (did not find in doc...)
 code refactoring
 test NSFW

** 13/07
 split video and image => creates 2 new services + separated conf for both
 code refactoring
 created AWS rekog client module
 add comments
 updated wiki

** 16/07
 code refactoring
 created pull request for rekog video
 started working on feature NSFW detection using gvision + rekog

** 17/07
 integrated nsfw detection using aws rekog

** 18/07
 need to find solution to :
 1)
   we need rekognition and gvision for language detection
   but we can't put rekognition in media worker AND we can't put language detection in rekog worker (bc language needs gvision which is media worker and bc we want language even if rekog module is disabled)

 2)
   nsfw service will sends 2 messages for the same media if both rekog and gvision both detect that the content is nsfw

 in video rekog : change labels res to Set of label (string) instead of List of labelResult

** 19/07
 create a visual analysis worker that will deal with rekog and gvision

** 20/07
 need to continue working on the worker that handles the whole visual analysis process
 send media id using jobTab (StartLabelDetectionRequest) instead of s3 video name
 start to follow rekog forum
 need to fix face area ratio > 1

** 23/07
 fixed nsfw check (not done twice) and language analysis (we have datas from rekog and gvision)
 updated pull request

** 24/07
 updated pull request
 integrated code into latest dev branch

** 26/07
 implemented face search api on a separate project for testing purpose
 found limitation :
   images we pass to indexFace must have a name that follow that regex: [a-zA-Z0-9_.\-:]+

** 27/07
 ran rekognition tests to check the quality of the search face feature
 updated the wiki

** 30/07
 start to code on the real project
 creating the store for the FaceId

** 31/07
 created face collection store
 search faces results now stored in aws collection
 create face search conf

** 01/07
 continue to implement face search feature

** 02/07
 changed model so have to reimplement what i did => code refactoring
 extractFacesInfoFromImage => facesInfo (ie the info about the person rekognized in the image) should be present in media
 searchFace + indexFace implemented with Placeholder VIP but not tested

** 03/07
 need to finish searchFace + indexFace then fix errors
 create VIP store

 indexFaces works => we store res in faceprint store
 search faces works => we store face info (bouding box + vip ID) in media.detectedFaces
 created VIP store

** 06/07
 changed findByAwsFaceId in FaceprintStore to only return faceprint which belong to the organization of the user
 created ManageVipService to handle addVip

** 07/07
 made rpc endpoint for VIP service

** 08/07
 completed rpc endpoint with index face + test => it works well

** 09/07
 ask what the security of the vip should be (ie: the one of the uploader or the one of the media )
 check that the security check works
 fix crop issue
 changed security check to use identity for face search

** 10/07
 code refactoring + bug fixing
 create method that will return bounding box on the media endpoint
 added web conf to integration3

 faced issue with ES where there was an indexation issue
 it was bc we changed labelsDetected from Opt[SDE] to Set
 to fix this, we reverted back to optSDE => it works now (ie we can safely delete the media)

** 13/07
 fixed issue with ES. bc we changed labelsDetected from Opt[SDE] to Set
 to fix this, we reverted back to optSDE => it works now (ie we can safely delete the media)
 + removed duplication

 updated branch to match latest dev branch
 added ffmpeg to my windows path to fix video request issue
 bug fixing

** 14/07
 better error handling +
 labels are deduplicated + fix ES schema error
 found + fixed NSFW check issue with video

** 16/07
 check that the image is big enough (more than 80 * 80)
 fixed bug where calling Search face when no face detected would cause complete rekognition failure (not just search face)
 check big photo (snail one seems not to work)
 check every rekog limitation for eventual issue

** 17/07
 updated pull request
 checked image transform from png -> jpeg with regis
 TODO :
 make searchFace works for video
   => complicated bc AWS returns a lot of useless info in the response
   => result are paginated so needs to read all page to get all the result...
 make sure that the id for the media and the vip are correct for indexVip (for now: we can give a media id instead of a vip id and it works)
 to get all results from face search https://forums.aws.amazon.com/thread.jspa?threadID=271079&tstart=50

** 20/07
 handle rekog pagination for video
 try to fix image croping issue with regis code

** 21/07
 fix image croping (graphiks magick issue)
 preparing for merge (updated env conf) + updated branch

** 22/07

** 23/07
 fixed croping
 fixed pagination
 removed noise from face search result to only keep the faces that matched => searchFace is now usable for videos

** 24/07
 changed the service that handle face collection operation (create coll, delete coll)
 check PR (server + chef)
 search bug
 merge PR into dev ?

 merge if regis reply
 collect Rekognition false positives

 check label on renault
 check width change
 check if  i calculate (aws_bounding_box -> normalized_rect) somewhere
 remove field searchedFaceBoundingBox for video
 test croping
 fix timestamp in label (aws_rekogn collection)
 FIX IMAGE CROPING
 FIX AREA RATION FOR VIDEO
 check that img/video are greater than AWs min size (more than 80 * 80 pixel)
 cron_restart chef PR
 add fix in https://bitbucket.org/sportaculous/sportaculous-server/pull-requests/156/441-bug-fix-impossible-to-create-branding/diff
   to vision worker

 08/10
 created the face collection on startup
 starting to work on Optim: we should not attempt any face recognition if zero face is recognized in the image
   => optim works for image

 09/10
 code refactoring =>
   pass enabled features in imageAnalysis in AWSRekognitionImageLocalService (same for videos)
   send a message to the worker once a feature is completed (label, text, ...)

 10/10
 tested optim (faceDetection prerequisite for faceSearch) and it works => made the PR
 started to work on "Allow to have distinct cloudfront uri expirations for videos and images"
   => made uri duration configurable with the conf
 updated my env file to fix issue when pulling latest web branch

 encountered a bug when downloading a media =>
   response from preflight is invalid
   net::ERR_SSL_PROTOCOL_ERROR
  this bug is on the int1/int3 env but not on the dev env (front)

** 11/10
*** continued working on Allow to have distinct cloudfront uri expirations for videos and images
   fixed response from preflight is invalid bug => ghostery
   updated byteground-util
   made the PR

** 12/10
*** started to work on When publishing a video, publish with Watermarks + add a 'in progress' state for posts
**** creating a polling service for further use
*** starting to refactor publishing service to include polling for every method that deal with transcoded content
*** should the publish method be made fire & forget ?

** 19/10
*** made the polling for open media stream method
*** refactor facebook, youtube & twitter service (bc they use that method)
*** polling works for image

** 22/10
*** make it work for video
*** impl time out

** 23/10 : publish with watermark
*** timeout works
*** YT works
*** test FB + twitt + YT
*** need google accounts right to post video on YT
*** ISSUE : localhost is on http and FB + Tw only accept HTTPS => can't test
*** started to work on Add retries when talking to Mandrill API

** 24/10
*** Finished Add retries when talking to Mandrill API
*** How to test it?
*** hard to code on bytegroud util because compile time is very long (hours) + computer is not responsive
 SEE for exception msg: https://mandrillapp.com/api/docs/messages.html

** 25/10
*** Add retries when talking to Mandrill API : we get the error msg
*** Need to add the retries + test

** 26/10
*** Fixed bug with taggedFaces in media (it's now indexed in ES)
*** got back on add watermark when publishing story
     => works for twitter / FB / content pool / YT

** 5 /11
*** pb with twitter + big video
 RPC[ERROR] Failed to finalize the chunked upload.
 RPC[ERROR] Relevant discussions can be found on the Internet at:
 RPC[ERROR]      http://www.google.co.jp/search?q=9254e8f7 or
 RPC[ERROR]      http://www.google.co.jp/search?q=00000127
 RPC[ERROR] TwitterException{exceptionCode=[9254e8f7-00000127 3fc6661e-000024c3], statusCode=-1, message=null, code=-1, retryAfter=-1, rateLimitStatus=null, version=4.0.6}
 RPC[ERROR]      at twitter4j.TwitterImpl.uploadMediaChunked(TwitterImpl.java:295)
 RPC[ERROR]      at com.sportaculous.service.TwitterPublicationHandlerService$$anonfun$com$sportaculous$service$TwitterPublicationHandlerService$$uploadMediaToTwitter$5$$anonfun$apply$2$$anonfun$apply$mcJJ$sp$1$$anonfun$apply$3.apply$mcJ$sp(TwitterPublicationHandlerService.scala:109)
 RPC[ERROR]      at com.sportaculous.service.TwitterPublicationHandlerService$$anonfun$com$sportaculous$service$TwitterPublicationHandlerService$$uploadMediaToTwitter$5$$anonfun$apply$2$$anonfun$apply$mcJJ$sp$1$$anonfun$apply$3.apply(TwitterPublicationHandlerService.scala:109)
 RPC[ERROR]      at com.sportaculous.service.TwitterPublicationHandlerService$$anonfun$com$sportaculous$service$TwitterPublicationHandlerService$$uploadMediaToTwitter$5$$anonfun$apply$2$$anonfun$apply$mcJJ$sp$1$$anonfun$apply$3.apply(TwitterPublicationHandlerService.scala:109)
 RPC[ERROR]      at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
 RPC[ERROR]      at scala.concurrent.package$.blocking(package.scala:123)
 RPC[ERROR]      at com.sportaculous.service.TwitterPublicationHandlerService$$anonfun$com$sportaculous$service$TwitterPublicationHandlerService$$uploadMediaToTwitter$5$$anonfun$apply$2$$anonfun$apply$mcJJ$sp$1.apply(TwitterPublicationHandlerService.scala:108)
 RPC[ERROR]      at com.sportaculous.service.TwitterPublicationHandlerService$$anonfun$com$sportaculous$service$TwitterPublicationHandlerService$$uploadMediaToTwitter$5$$anonfun$apply$2$$anonfun$apply$mcJJ$sp$1.apply(TwitterPublicationHandlerService.scala:107)
 RPC[ERROR]      at com.byteground.io.package$.using(package.scala:161)
 RPC[ERROR]      at com.sportaculous.service.TwitterPublicationHandlerService$$anonfun$com$sportaculous$service$TwitterPublicationHandlerService$$uploadMediaToTwitter$5$$anonfun$apply$2.apply$mcJJ$sp(TwitterPublicationHandlerService.scala:107)
 RPC[ERROR]      at com.sportaculous.service.TwitterPublicationHandlerService$$anonfun$com$sportaculous$service$TwitterPublicationHandlerService$$uploadMediaToTwitter$5$$anonfun$apply$2.apply(TwitterPublicationHandlerService.scala:106)
 RPC[ERROR]      at com.sportaculous.service.TwitterPublicationHandlerService$$anonfun$com$sportaculous$service$TwitterPublicationHandlerService$$uploadMediaToTwitter$5$$anonfun$apply$2.apply(TwitterPublicationHandlerService.scala:106)
 => not possible to upload video more than 2min to twitter => created story

*** created conf for polling service
*** trying to implement time out for polling service

** 6/11
*** fixed timeout
*** made PR
*** added a limitation to twitter video (can'(t be more than 2min 20)
*** got back on add retries when sending mail with mandrill
**** very slow testing process

** 7/11
*** completed add retries when talking to mandrill api story => made PR
*** found a bug in NSFWWarning local service (where exception that occured during the sending of warning emails were not thrown)
*** started to work on support for nsfw plugin

** 8/11
*** took note of the comments on my PR
 => changed code in Mandrill retry
 => run test,it works
*** added support for nsfw plugin

** 9/11
*** test OK for support for nsfw plugin => made the PR
*** started to work on When indexing a vip, detect if the bounding box is too small and return appropiate error
 => lots of code refactoring
 => created plugin to disable/enable face detection
 => changed RPC return value of indexVip to indicate whether the newly indexed face will take part in AWS Face Rekognition

** 16/11
*** added conf timeout/period for polling in publication service
*** refactored  publicationService
*** merged the PR : publish for video
*** continued working on madrill retries

** 19/11
*** updated PR for mandrill retries
*** started to work on "on the back side, adapt getFaceBoundingBoxes for GVISION"
**** more complicated than I thought, because GVision data are not normalized unlike AWS one
**** also, we crop image before sending them to Google, so we can't use the media width * height to normalize the Gvision bounding box
**** SOLUTION : we store crop info in gvision Data
     => code refactoring on google vision service
     => now we store media width and height => I can normalize GVision bounding box & use them along Rekog bounding box

** 20/11
*** we now gets both AWS bounding box & normalized gvision bounding box
*** changed rpc response for getBoundingBox
*** fixed an issue with AWS rekognition timestamp for label detection on video
*** need to handle special case with GVISION where boundingBox is at the edge of the image (API doesn't return every BB value)
*** => its working => made the PR

** 21/11
*** changed how we store gvision datas => we don't store mediaWidth & mediaHeight, but we calculate normalized bounding box and store it directly
*** spent lot of time with GVision api to access normalized boundingBox directly => no result

** 22/11
*** merged mandrill story into dev
*** continue working on adapting gvision boundingBox
*** created a json parser that normalize all boundingBox fields

** 23/11
*** successfully normalized boundingBox + stored in media
*** adaptated rpc response to getFaceBoundingBoxes
*** made add Gvision boundingBox PR
*** worked on When indexing a vip, detect if the bounding box is too small and return appropiate error
*** made the PR

** 30/11
*** code refactor to separate worker message from rekognition service (which should only return value instead of sending msg)
*** => awsService doesn't send message
*** started to work on Extend getFacesBoundingBox API endpoint to return any GVision/Rekognition data

** 3/12
*** Extended getBoundingBox rpc endpoint to return boundingBox angle info
*** now returning text + boundingBox for text in vision + boundingBox for face in vision

** 4/12
*** adaptated model + code to get new rekog data (object detection + boundingBox)
*** now gets LOGO + LABEL objets boundingBox
*** merged PR about normalizing GVision boundingBox

** 5/12
*** test + update PR for detect if boundingBox is too small + visual plugin
*** made PR for story about change coreCreatedDate to wordCreated for doc

** 6/12
*** work on : Face reco: only register recognitions for vips that we have access to

** 7/12
*** migration Gvision => normalized boundingBox
*** add to track detectedFaces bug
*** test for vip + faceprint access during set detectedFace in media
*** spent some time trying to understand rekog behaviour : if we indexFace on the same img two times, AWS will returns two identical awsFaceId

** 20/12
*** started to work on : Do not fail when indexing a vip from a bounding box containing more than one face
*** test => it works => made PR => only one face (the biggest) get indexed when indexing an image
*** added cover id field to vip
*** made hotfix to fix infinite exception throwing in vision worker

** 21/12
*** deployed hotfix (fix infinite exception throwing in vision worker )
*** remove register & updateSelfPassword from account API
*** remove create/update/delete sur le endpoint "activity"
*** started to work on Add retries when accessing download server from workers and api

** 31/12
*** works on detecting slow mo in video
**** read articles + learning openCV / javaCV

** 04/01
*** starting to implement prototype
*** it finds the logo !

** 07/01
*** fixed issue with detect only one face PR
*** fixed an issue where multiface search would not check for image crop size (less than 80x80 px)
*** needed to make change in RekogImageService and check crop img size before sending searchFace request for multi searchFace request
*** issue with graphiks magick cmd

** 08/01
*** fixed issue where face crop size was not checked to be more than 80 * 80 px + merged PR
*** fixed issues with story "Do not fail when indexing a vip from a bounding box containing more than one face" + merged PR
*** need to finish remove deprecated from activity record endpoint

** 21/01
*** use GCP to detect replays
*** trained model to recog logos => it works really well !

** 22/01
*** replay detection has good result (but a bit expensive)
*** looked into open source / free Computer vision solution : can be used to detect people + ball

** 23/01
*** look into color detection : track players and tag them with the color of their team

** 25/01
*** started to get good results with the openCV approach (80% of the logos are detected)
*** still some logo missing and false positive

** 29/01
*** tune hypeparameters +
   val binSize = 256
   val threshold1 = 0.08
   val threshold2 = 10d

** 18/03
*** started to work on FB analytics

** 19/03
*** can't get analytics
  cred issue ?
  SDK issue ? version ?
  code issue ?
*** need to do pagination to get the results
 Second you need to return all comments/likes by adding a "filter(stream)" param to your call
 for comments or likes. Facebook "hides" some comments or likes due to a low "story" value.
 If you only want the total number of likes, but not the individual likes - then set limit to 0, and summary to 1

** 20/03
*** worked on getting Insight data + deserializing them

 PB : SDK only ? no like/share for video + no share for image

** 25/03
*** code review
*** worked on creating activity record for FB stat

** 26/03
*** continue creating activity record
*** lost time on issue for serializing
*** need to fix delta only

** 27/03
*** done activity record for FB stat (+ insights)
*** added insights stats
*** code review (add index to mongo collection)
*** changed a field (mediaId -> [mediaIds]) in the activity record of publication analytics
 need to do change some code in youtube analytics service (plus migration after)

** 29/03
*** need to work on calc delta for FB stat + absolute stat for YT
*** refactored code
*** FB stats delta in activity record

** 01/04
*** faced sdk limitation (no shares / likes for video, only comments)
*** got videos insights
*** got shares /likes /comments for all kind of publication

** 03/04
*** review major changes in FB graph API
*** updated to 2.12 (last 2.x version) and tested => found no issue
*** need to fix issue with video insight
*** Issue : when publish to FB => publication-analytics failed - caused by java.util.NoSuchElementException: Future.filter predicate is not satisfied
 need to check video insight

** 05/04
*** need to upgrade FB version for publication service
*** check if insights work only bc its a test page
*** set up dev env on new pc ... (whole day)

** 07/04
*** fixed new pc IDE issue
*** code review
*** updated branch to latest dev (1 year old)

** 08/04
*** test :
	 - bundle
	 - image
	 - doc
	 - place where facebook4j
	 - lookUpPlaceId
	 - check multiple external account publ
*** fixed : Future.filter issue

** 9/04
*** DONE meeting with benoit + alex + regis => need to find good activity record, because :
	 - is it possible to aggregate the field of multiple records in ES
		 => publ = img1 + img2 => tot_view = img1.view + img2.view)
	 - we skip generating activity record for media (in pub) where there is no change
		 => impossible to generate REAL absolute value for publi (missing values)
*** DONE adding post (multiple image pub) stats
*** fix :
	 - DONE mediaIds -> mediaId (extractSocialMediaAnalytics)
*** check :
	 - select all

** 10/04
*** need to find solution for API rate limiting issue
	 => only 200 calls per user
	 - Quotes :
	 "If your app is making enough calls to be considered for rate limiting by our
	 system, we return an X-App-Usage HTTP header'
	 " So, if you don't get any X-App-Usage header,Then your app hasn't be
	 considered "worthy" of throttling by their automated systems yet.
	 So it would be best to check for this header, while making your api requests.
	 Once you start receiving this Header, it would be best change your frequency
	 of the API calls or give a timeout."
*** fix :
	 - wrong type (video) for multi image FB publication
	 - need to stored post impression in downloadMedia activity

** 11/04
*** investigated API rate limit
*** investigated FB app token
*** reduced API calls in code
*** added flag to turn off FB analytics
*** PB :
	 - we store user token instead of page token when linking account

** 12/04
*** DONE store page token
	 - external account impl facebookPageFindPages après getAccount.asScala
*** déterminer si faire un appel user token -> page token consomme un API call

** 15/04
*** added viewsCount + viewsCountDelta for FB
*** added viewsCount abs for YT
*** started to work on adding post count to media inside post count

** 16/04
*** done generating correct downloadMedia acti record (fbPostId, subtype), for FB only, needed for YT?
*** no shares count => FIXED
*** removed insights calls for media inside post (same as message post insights)

** 17/04
*** update branch to latest dev
*** added page usage check
*** test
*** PR

** 23/04
*** check if it's possible to use the FB multi language post feature from the DAM
	 in FB, it's possible to publish a message in different language (one "line" for each language)
	 then, FB will display the correct language in function of the language of each FB user that see the post
	 https://www.facebook.com/help/181155025579876?helpref=related
*** add FB activity record to wiki page
*** continue testing

** 24/04
*** mandrill log PR
*** worked on Instrument ElasticSearch client to log/record slow queries

** 25/05
*** made worked on Instrument ElasticSearch client to log/record slow queries PR
*** facebook chef PR


*** DONE Media.getBoundingBoxes: allow to batch the retrieval of bounding boxes for several medias PR
*** DONE Add new MongoDB indexes in MongoDB store init

** 26/05
*** Photo import: favor takenDate from IPTC rather than exif if both are present PR
*** worked on Extract timezone from exif 2.31 version
	 - built custom metadata-extractor lib version

** 30/05
*** worked on adding request name + id to log slow ES query
*** how to get ES query as JSON in profiling proxy
*** BYTEGROUND util 129.0.0 release
*** how to generate API doc for getBoundingBoxes

** 03/05
*** need to fix FB publication error in FB analytics branch
*** updated exif story branch to match dev changes

** 06/05
*** investigated ES request time (how to deal with refresh block time)
*** facebook deprecated multiple endpoints
	 - deprecated an endpoint we use for publishing

** 07/05
*** worked on extracting EXIF timezone info (now we store them inside the date, not separately)
	 - made the PR
*** try to find why FB issue (can't publish)
	 - only blocked for newer page on my branch

** 08/05
*** merged favor takenDate from IPTC rather than exif if both are present PR
*** talk about exif timezone with regis and alex
*** continued investigating about FB issue
	 - issue with insights right
	 - DEPRECATED LIST :
		 - GET /{photo-id}/sharedposts
*** made some fix to EXIF timezone story

** 13/05
*** worked on migration exif to iptc

** 14/05
*** worked on migration exif to iptc (takenDate)
	 - discovered IPTC ( issue
*** worked on fixing IPTC issue (hotfix 4.15.3)

** 15/05

*** worked on Change the extraction process for coreDatesTakenAt
   - made hotfix
   - check if it should be merged in dev or master (currently dev)

** 17/05
*** worked on exif timezone
	 - made a byteground util release
	 - then merged PR

** 20/05
*** started to investigate on BUGG 445 Album and Location feature works only with image when publishing to FB
	 - seems to be a missing feature in FacebookPublicationHandlerService (see comment l.478)
	 - editing once the video is uploaded : place is deprecated ({"error":{"message":"(#12) place is deprecated for versions v2.9 and higher","type":"OAuthException","code":12,"fbtrace_id":"FgFT2qzps5z"}})
	 - during upload : upload is a blackbox (facebook4j doesn't accept the place parameter)

*** found bug in FB pub
	 - we use app token to look up for place id (france, germany, ...)

*** hotfix : Exif, use datetime_original instead of datetime

** 21/05
*** FB
	 - solution : make post (placeholder) then upload video into it

  $


** 22/05
*** iptc migration
	 - sol : create 1*1 img with the metadata using graphics magick (or ffmpeg)
	 - appeler le download server (stocké dans un bucket metadata diff) pour créer et uploader le fichier metadata vers s3
	 - ne pas permettre que le endpoint "metadata" soit accessible depuis l'extérieur
	 - unsecureddownloadrpc

*** starting

  to add upload/download metadata file methods
   on download server
   - ajouter putMetadata/getMetadata méthodes dans MediaStreamLocalService.scala  (écrit/lit AWS)
   - TODO : find all the cases where we use ImageRequest.transformed
			 -  test depuis le browser

** 27/05
*** DONE
	 - GM crops now works
	 - store IPTC

 	 version in media.attributes

*** FIXED AWS sdk issue

** 28/05
*** store EXIF version in media.attributes
*** made quick fix about EXIF metadata extraction causing crash of the metadata extractor
*** started to work on the migration
	 - recalc coreDatesTakenAt
	 - recalc iptcCoreImageDateCreated
	 - faire des log du plus vieux au plus récent


** 29/05
*** continued working on the migration code

** 31/05
*** DONE faire la fonction qui va récupérer les média pas updater
*** DONE tag not updated : systemTag
*** DONE in (batch_size) -> (mediaId updated, mediaId failed)
*** read PR Support editing the rotation of a media
*** made PR

** 03/06
*** made some fix on migration code (changed how we treat batch to not crash download server)
*** added method that calc NB left to migrate

** 04/06
*** DONE make script that will call RPC endpoint
*** DONE fix conflict

** 05/06
*** DONE testing migration (no pb found)

** 10/06
*** got back on FB analytics story
*** fix
   ed publication bug (is_published instead of published)
*** app token consumption when using fb search place
	 - dont know if the app access token is used in place lookup bc it's the dev env
	 - doesn't raise when using page access token...
	 - app token used in staging env !!!
	 - tested analytics code

** 11/06
*** updated code to match latest dev branch
*** FIXED important bugs where we stored the wrong (first in the account) page token during publication
*** FIXED app token usage in places search API

** 12/06
*** BUG connecting external account use app token
	 - app consumption BEFORE facebookPageLinkToPage and in readFacebookPageAccountInfos
	 - FOUND that user token use count toward App throttling
*** start to work on the migration script
*** tested the feature : it works good

** 13/06
*** start the migration script

** 14/06
*** finish extractSocialMediaAnalytics migration
*** start downloadMedia migration
*** finish migration

** 17/06
*** verif scheduling
*** migration :
	 - delete activity records
	 - Es migration
*** start to work on Photo import: extract timezone info from "maker" group if not iptc are available
	 - made inventory of the relevant fields for each camera company
	 - need to see this with benoit

** 19/06
*** work on maker timezone
	 - decided not to tweak lib
	 - but instead use the raw maker field with the field id
	 - done for canon

** 21/06
*** extract tz for : canon / nikon / almost sony

** 24/06
*** found bug with Nikon (needed to read the bytes in the correct order ie big or little endian)
*** fixed byte order issue

** 25/06
*** fixed FB bug (store page access token after pub/analytics)

** 26/06
*** update code match last dev branch for Migration: Re-extract "iptcCoreImageDateCreated" to take time into account (not just date) + Metadata generation
	 + test => works $*



*** worked on send email notif first con shared gallery
	 - created method endpoint widget
	 - created placeholder mandrill template

** 27/06
*** need to retrieve country/browser/os from ES plugin
*** trying to use ES module with java client
*** finish template

** 28/06
*** retrieve res from ES
*** made the request async
*** finish en_US template

** 01/07
*** finished testing shared gallery notif mail
*** need to wait for default value for deserialization before merge
*** match dev branch for metadata generation (uplaod to s3 etc)
	 - test code

** 02/07
*** need to further test fix conflixt
*** need to include Régis modif in migration script

*** merged FB analytics
*** fixed various issues with first notification PR + merged PR
*** updated code for timezone related stories

** 03/07
*** post merge fixes
*** deleted corrupted analytics in ES/publ/scheduled_message
*** deleted corrupted analytics in ES/publ/scheduled_message

** 05/07
*** run FB migration (act record) mongo + ES
*** done migration on int/dev/sta

** 08/07
*** start to work on detected branding support
	 - created the model (media etc)
	 - created the stored

** 09/07
*** retrieve logo from GVision & store it in mongo

** 11/07
*** complete :
	 brandprint



	 brand in media
	 brand as asset
*** retrieve regis branch about Vision worker and fixed conflicts

** 15/07
*** need to implement "model.discardedBrands" & "model.softDeletedBrands"
*** create rpc method indexBrand
*** started to work on onBrandDeletionStatusChange

** 16/07
*** check that override def discardByPrimaryKeys(primaryKeys: Iterable[Asset.Id], date: Date = new Date) delete brand info in media etc
*** maybe refactor code for brand/vip
*** done unindexBrand
*** worked on FB refresh album list bug
	 + fixed it

** 19/07
*** merged latest dev + regis branch
*** run final test + done fixes

** 22/07
*** changed RPC method indexBrand to take as param asset id instead of asset name

*** blocking issue in serialization framework

** 23/07
*** added thumbnail to detected brand (dont work)
*** when brand is deleted , it is no longer visible for future detection
*** make PR

** 24/07
*** make PR for better 1st connection to shared gallery
*** worked on FB bug with Change facebook avatar profile url retriveal
	 + made PR (solution : use graph URL instead of CDN)
	 + migration (done on integration)

** 25/07
*** DONE enhance FB bug migration
*** DONE lots of fixes and refactoring for brand detection
*** DONE do the actual FB bug migration
*** DONE do look alike for brand

** 26/07
*** DONE fb bug migration
*** DONE continue fixing brand detection
	 + DONE make it more like face detection
	 + DONE remove incoherences
	 + DONE code refactoring
	 + DONE fix issue where only one brand is put in media
	 + DONE do lookalike brand and brand aliasing
	 + DONE (but not tested) do lookalike brand !!

** 29/07
*** DONE run test (works for lookalike + alias)

** 30/07
*** DONE fix copy
*** DONE search by name
*** DONE store by MID
** 01/08
*** DONE investigate FB @user completion
    CLOSED: [2019-08-01 jeu. 14:29]
**** impossible to do
 FB doesn't allow acces to search user anymore
 [[https://developers.facebook.com/docs/graph-api/changelog/breaking-changes#search-4-4][facebook ref]]
*** DONE update youtube api version
    CLOSED: [2019-08-02 ven. 12:44]
 Going from v3-rev183 (avril 2017) to v3-rev212 (mai 2019). Requires updating BOTH Youtube and GVision

**** Youtube
 Changelist : [[https://developers.google.com/youtube/v3/revision_history#may-17-2017][here]]
 Check :
***** DONE publish to youtube
      CLOSED: [2019-08-01 jeu. 16:35]
***** DONE retrieve analytics from youtube pub
***** VideoSnippet
***** VideoStatus
***** videoInsertionRequest.getMediaHttpUploader.setDirectUploadEnabled
 Déprecation :
***** The video resource's recordingDetails.location.altitude
***** YouTube is removing support for the Featured Video and Featured Website featureswhich are supported in the API via the channel resource's invideoPromotion object

**** GVision
 Changelist : [[https://cloud.google.com/vision/docs/release-notes][here]]
 Works fine
*** DONE remove unused facebook metrics worker and service
    CLOSED: [2019-08-02 ven. 12:49]

** 02/08
*** TODO investigate twitter @user completion
*** TODO facebook multi photo post on album
**** how will analytics work with duplicated photos (album + page)

**** it works, but needs to investigate if side effect
**** seems to be issue with post msg (gets on all the img msg)
